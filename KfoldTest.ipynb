{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "\n",
    "import numpy as np\n",
    "from math import *\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random \n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = ['Rough','UTS','Elon']\n",
    "index = 2\n",
    "CASE = folders[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prepross import importForK\n",
    "x,y = importForK(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lossfn = [tf.keras.losses.MeanSquaredError(),tf.keras.losses.MeanAbsolutePercentageError(),tf.keras.losses.MeanAbsoluteError()]\n",
    "lossfn = lossfn[index]\n",
    "optimizer = [tf.keras.optimizers.Adam(learning_rate=i,beta_1=0.09,beta_2=0.9) for i in [5e-2,1e-2,5e-3]]\n",
    "optimizer = optimizer[index]\n",
    "metrics = ['mean_squared_error','mean_absolute_error',tf.keras.losses.MeanAbsolutePercentageError()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models\n",
    "files = [f for f in os.listdir(CASE) if f.startswith(CASE) and f.endswith('k.h5')]\n",
    "files.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeElex(a):\n",
    "    a = np.array(a)\n",
    "    temp=a[:,0,:]\n",
    "    a[:,0,:]=a[:,-1,:]\n",
    "    a[:,-1,:]=temp\n",
    "    return np.array(a)\n",
    "\n",
    "def changeEley(a):\n",
    "    a = np.array(a)\n",
    "    temp=a[:,0]\n",
    "    a[:,0]=a[:,-1]\n",
    "    a[:,-1]=temp\n",
    "    return np.array(a)\n",
    "    \n",
    "xtrain =[]\n",
    "xtest = []\n",
    "ytrain =[]\n",
    "ytest = []\n",
    "n_split = 10\n",
    "for train_index,test_index in KFold(n_split).split(x):\n",
    "    xtrain.append(x.values[train_index])\n",
    "    xtest.append(x.values[test_index])\n",
    "    ytrain.append(y.values[train_index])\n",
    "    ytest.append(y.values[test_index])\n",
    "xtrain = changeElex(xtrain)\n",
    "xtest = changeElex(xtest)\n",
    "ytrain = changeEley(ytrain)\n",
    "ytest = changeEley(ytest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tperformance = {}\n",
    "performance = {}\n",
    "\n",
    "for i,f in enumerate(files):\n",
    "    if i%10==0:\n",
    "        arch = f[len(CASE)+1:-18]\n",
    "    else:\n",
    "        arch = f[len(CASE)+1:-17]\n",
    "    performance[str(arch)] = {\n",
    "        'MSE':0,\n",
    "        'MAE':0,\n",
    "        'MRE':0\n",
    "    }\n",
    "    tperformance[str(arch)] = {\n",
    "        'MSE':0,\n",
    "        'MAE':0,\n",
    "        'MRE':0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models ={}\n",
    "for i,f in enumerate(files):\n",
    "    name = f\n",
    "    if i%10==0:\n",
    "        arch = f[len(CASE)+1:-18]\n",
    "        print(i/10+1)\n",
    "    else:\n",
    "        arch = f[len(CASE)+1:-17]\n",
    "    v = f[-15:-13]\n",
    "    models[str(f)] = tf.keras.models.load_model(os.path.join(CASE+\"/\"+f),compile=False)\n",
    "    models[str(f)].compile(\n",
    "        optimizer = tf.optimizers.Adam(learning_rate=1e-2, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name='Adam'),\n",
    "        loss = lossfn,\n",
    "        metrics = ['mean_squared_error','mean_absolute_error',tf.keras.losses.MeanAbsolutePercentageError()]\n",
    "    )\n",
    "    perf = models[str(f)].evaluate(xtest[i%10],ytest[i%10], verbose=0)\n",
    "    tperformance[str(arch)]['MSE']+=perf[1]\n",
    "    tperformance[str(arch)]['MAE']+=perf[2]\n",
    "    tperformance[str(arch)]['MRE']+=perf[3]\n",
    "    \n",
    "    perf = models[str(f)].evaluate(x,y, verbose=0)\n",
    "    performance[str(arch)]['MSE']+=perf[1]\n",
    "    performance[str(arch)]['MAE']+=perf[2]\n",
    "    performance[str(arch)]['MRE']+=perf[3]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort by Performence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_getter(item):\n",
    "    return item[1][\"MAE\"]\n",
    "sortP = sorted(tperformance.items(),key=value_getter)\n",
    "sortO =sorted(performance.items(),key=value_getter)\n",
    "print('Performance on test',sortP[0])\n",
    "print('Performance on df',sortO[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNamesP = [f for f in models.keys() if f.startswith(CASE+\"@\" + sortP[0][0]) and f.endswith('k.h5')]\n",
    "modelNamesO = [f for f in models.keys() if f.startswith(CASE+\"@\" + sortO[0][0]) and f.endswith('k.h5')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfO = {}\n",
    "perfP = {}\n",
    "for m in modelNamesP:\n",
    "    model = models[m]\n",
    "    perfP[m] = model.evaluate(x,y,verbose=0)\n",
    "\n",
    "for m in modelNamesO:\n",
    "    model = models[m]\n",
    "    perfO[m] = model.evaluate(x,y,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_getter(item):\n",
    "    return item[1][0]\n",
    "sortPP = sorted(perfO.items(),key=value_getter)\n",
    "sortOO =sorted(perfP.items(),key=value_getter)\n",
    "print('Performance on test',sortPP[0])\n",
    "print('Performance on df',sortOO[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pmodel = models[sortPP[0][0]]\n",
    "vP = int(sortPP[0][0][-14:-13])\n",
    "Omodel = models[sortOO[0][0]]\n",
    "vO = int(sortOO[0][0][-14:-13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 2,1\n",
    "fig, gs = plt.subplots(rows, cols,figsize =(10,10),sharex='col',sharey='row')\n",
    "\n",
    "for i,(mod,ve) in enumerate(([Pmodel,vP],[Omodel,vO])):    \n",
    "    ypred_tr =  mod.predict(xtrain[ve],verbose=0).squeeze()\n",
    "    ypred_te = mod.predict(xtest[ve],verbose=0).squeeze()\n",
    "    ax = fig.add_subplot(gs[i])\n",
    "    ax.plot(y,y,'r')\n",
    "    ax.plot(ytrain[ve],ypred_tr,'bo',label=\"Train\")\n",
    "    ax.plot(ytest[ve],ypred_te,'ko',label = 'Test')\n",
    "    ax.legend()\n",
    "    plt.title(mod.name)\n",
    "    plt.xlabel(CASE)\n",
    "    plt.ylabel(CASE +' Prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a8477db803d2ebe70aa0971410c968e2c89bebd74bd07aee6f225349be8a6b47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
