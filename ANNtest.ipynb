{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "\n",
    "import numpy as np\n",
    "from math import *\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = ['Rough','UTS','Elon']\n",
    "index = 1\n",
    "CASE = folders[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prepross import LoadAndProcess\n",
    "xtrain,xval,xtest,ytrain,yval,ytest = LoadAndProcess(index)\n",
    "x = pd.concat((xtrain,xtest,xval),axis=0)\n",
    "y = pd.concat((ytrain,ytest,yval),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lossfn = [tf.keras.losses.MeanSquaredError(),tf.keras.losses.MeanAbsolutePercentageError(),tf.keras.losses.MeanAbsoluteError()]\n",
    "lossfn = lossfn[index]\n",
    "optimizer = [tf.keras.optimizers.Adam(learning_rate=i,beta_1=0.09,beta_2=0.9) for i in [5e-2,1e-2,5e-3]]\n",
    "optimizer = optimizer[index]\n",
    "metrics = ['mean_squared_error','mean_absolute_error',tf.keras.losses.MeanAbsolutePercentageError()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "files = [f for f in os.listdir(CASE) if f.startswith(CASE) and not f.endswith('k.h5')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  tf.keras.models.load_model(os.path.join(CASE+\"/\"+files[0]),compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "models = {}\n",
    "val_performance = {}\n",
    "performance = {}\n",
    "ove_performance = {}\n",
    "for i in files:\n",
    "    print(i)\n",
    "    models[str(i)] = tf.keras.models.load_model(os.path.join(CASE+\"/\"+i),compile=False)\n",
    "    models[str(i)].compile(\n",
    "        optimizer = tf.optimizers.Adam(learning_rate=1e-2, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name='Adam'),\n",
    "        loss = lossfn,\n",
    "        metrics = metrics\n",
    "    )\n",
    "    val_performance[str(i)] = models[str(i)].evaluate(xval,yval,verbose=0)\n",
    "    performance[str(i)] = models[str(i)].evaluate(xtest,ytest, verbose=0)\n",
    "    ove_performance[str(i)] = models[str(i)].evaluate(x,y, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_getter(item):\n",
    "    return item[1][2]\n",
    "sortP = sorted(performance.items(),key=value_getter)\n",
    "sortV = sorted(val_performance.items(),key=value_getter)\n",
    "sortO =sorted(ove_performance.items(),key=value_getter)\n",
    "print('Performence on test',sortP[0])\n",
    "print('Performence on val',sortV[0])\n",
    "print('Performence on df',sortO[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pmodel = tf.keras.models.load_model(os.path.join(CASE+\"/\"+sortP[0][0]),compile=False)\n",
    "Pmodel.compile(optimizer = optimizer,loss = lossfn,metrics=metrics)\n",
    "Vmodel = tf.keras.models.load_model(os.path.join(CASE+\"/\"+sortV[0][0]),compile=False)\n",
    "Vmodel.compile(optimizer = optimizer,loss = lossfn,metrics=metrics)\n",
    "Omodel = tf.keras.models.load_model(os.path.join(CASE+\"/\"+sortO[0][0]),compile=False)\n",
    "Omodel.compile(optimizer = optimizer,loss = lossfn,metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sortO[:10]:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sortP[:10]:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 3,1\n",
    "fig, gs = plt.subplots(rows, cols,figsize =(10,20),sharex='col',sharey='row')\n",
    "\n",
    "for i,mod in enumerate([Pmodel,Vmodel,Omodel]):    \n",
    "    ypred_tr =  mod.predict(xtrain,verbose=0).squeeze()\n",
    "    ypred_te = mod.predict(xtest,verbose=0).squeeze()\n",
    "    ypred_va = mod.predict(xval,verbose=0).squeeze()\n",
    "    ax = fig.add_subplot(gs[i])\n",
    "    ax.plot(y,y,'r')\n",
    "    ax.plot(ytrain,ypred_tr,'bo',label=\"Train\")\n",
    "    ax.plot(ytest,ypred_te,'ko',label = 'Test')\n",
    "    ax.plot(yval,ypred_va,'go',label='Validation')\n",
    "    ax.legend()\n",
    "    plt.title(mod.name)\n",
    "    plt.xlabel(CASE)\n",
    "    plt.ylabel(CASE +' Prediction')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypredtr =  Omodel.predict(xtrain).squeeze()\n",
    "ypredts = Omodel.predict(xtest).squeeze()\n",
    "ypredvl = Omodel.predict(xval).squeeze()\n",
    "\n",
    "y = [*ytrain,*ytest,*yval]\n",
    "plt.plot(y,y,'r')\n",
    "plt.plot(ytrain,ypredtr,'bo',label=\"Train\")\n",
    "plt.plot(yval,ypredvl,'ro',label=\"val\")\n",
    "plt.plot(ytest,ypredts,'ko',label = 'Test')\n",
    "plt.legend()\n",
    "plt.xlabel(CASE)\n",
    "plt.ylabel(CASE +' Prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Performance on Validation Dataset:')\n",
    "Omodel.evaluate(xval,yval)\n",
    "print('Performance on Test dataset:')\n",
    "Omodel.evaluate(xtest,ytest)\n",
    "print('Performance on Train Dataset:')\n",
    "Omodel.evaluate(xtrain,ytrain)\n",
    "print('Performance on Dataset:')\n",
    "Omodel.evaluate(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(CASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "layerC = { 'tanh' : [0,0,0],\n",
    "        'linear' : [0,0,0],\n",
    "        'sigmoid' : [0,0,0],\n",
    "        'relu' : [0,0,0]}\n",
    "layerS = { 'tanh' : [0,0,0],\n",
    "        'linear' : [0,0,0],\n",
    "        'sigmoid' : [0,0,0],\n",
    "        'relu' : [0,0,0]}\n",
    "def getLayers(i):\n",
    "    i = i.split('_')\n",
    "    i[0] = ''.join(re.split(\"[^a-zA-Z]*\", i[0][len(CASE):]))\n",
    "    for n in len(i[1:]):\n",
    "        i[n] = ''.join(re.split(\"[^a-zA-Z]*\", i[n]))\n",
    "    i = i[:-2]\n",
    "    return i\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchBestNum = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sortO[:searchBestNum]:\n",
    "    layers = getLayers(i[0])\n",
    "    for i,l in enumerate(layers):\n",
    "        layerC[l][i] +=1\n",
    "        layerS[l][i] += performance[i[0]][0]\n",
    "        \n",
    "for l in layerS.keys():\n",
    "    for i,c in enumerate(layerS[l]):\n",
    "        layerS[l][i] = c / (layerC[l][i]+ 1e-10) \n",
    "layerS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oschemaScore = {}\n",
    "pschemaScore = {}\n",
    "schemasCount = {}\n",
    "from architectures import getSchemas\n",
    "schemas = getSchemas(['tanh','sigmoid','relu','linear'])\n",
    "\n",
    "for i in sortO[:searchBestNum]:\n",
    "    layers = getLayers(i[0])\n",
    "    lname = ''\n",
    "    for j in layers:\n",
    "        lname+=j\n",
    "    pschemaScore[lname] = 0\n",
    "    oschemaScore[lname] = 0\n",
    "    schemasCount[lname] = 0\n",
    "for i in sortO[:40]:\n",
    "    layers = getLayers(i[0])\n",
    "    lname = ''\n",
    "    for j in layers:\n",
    "        lname+=j\n",
    "    pschemaScore[lname] += performance[i[0]][0]\n",
    "    oschemaScore[lname] += ove_performance[i[0]][0]\n",
    "    schemasCount[lname] += 1\n",
    "\n",
    "for name in oschemaScore.keys():\n",
    "    oschemaScore[name] = oschemaScore[name]/schemasCount[name]\n",
    "    pschemaScore[name] = pschemaScore[name]/schemasCount[name]\n",
    "\n",
    "def value_getter(item):\n",
    "    return item[1]\n",
    "bSchemaO = sorted(oschemaScore.items(),key=value_getter)\n",
    "bSchemaP = sorted(pschemaScore.items(),key=value_getter)\n",
    "bSchemaP[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bSchemaO[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a8477db803d2ebe70aa0971410c968e2c89bebd74bd07aee6f225349be8a6b47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
